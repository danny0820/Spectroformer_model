# Spectroformer æå¤±å‡½æ•¸è©³ç´°å ±å‘Š

## ğŸ“‹ ç›®éŒ„

1. [ç¸½é«”æ¦‚è¿°](#ç¸½é«”æ¦‚è¿°)
2. [æå¤±å‡½æ•¸æ¶æ§‹åœ–](#æå¤±å‡½æ•¸æ¶æ§‹åœ–)
3. [å„æå¤±å‡½æ•¸è©³è§£](#å„æå¤±å‡½æ•¸è©³è§£)
   - [Charbonnier Loss](#1-charbonnier-loss)
   - [VGG Perceptual Loss](#2-vgg-perceptual-loss)
   - [Gradient Loss](#3-gradient-loss)
   - [MS-SSIM Loss](#4-ms-ssim-loss)
4. [å‹•æ…‹åŠ æ¬Šæ©Ÿåˆ¶](#å‹•æ…‹åŠ æ¬Šæ©Ÿåˆ¶)
5. [æ•¸å­¸æ¨å°èˆ‡å…¬å¼](#æ•¸å­¸æ¨å°èˆ‡å…¬å¼)
6. [æå¤±å‡½æ•¸æ¯”è¼ƒè¡¨](#æå¤±å‡½æ•¸æ¯”è¼ƒè¡¨)
7. [è¨“ç·´ä¸­çš„æå¤±è¡Œç‚ºåˆ†æ](#è¨“ç·´ä¸­çš„æå¤±è¡Œç‚ºåˆ†æ)
8. [è¶…åƒæ•¸å»ºè­°](#è¶…åƒæ•¸å»ºè­°)
9. [ç¨‹å¼ç¢¼å¯¦ç¾è©³è§£](#ç¨‹å¼ç¢¼å¯¦ç¾è©³è§£)

---

## ç¸½é«”æ¦‚è¿°

Spectroformer è¨“ç·´è…³æœ¬ (`trains.py`) æ¡ç”¨ **å¤šæå¤±çµ„åˆç­–ç•¥**ï¼Œçµåˆå››ç¨®ä¸åŒçš„æå¤±å‡½æ•¸ä¾†å„ªåŒ–æ°´ä¸‹å½±åƒå¢å¼·ä»»å‹™ã€‚é€™ç¨®å¤šæå¤±è¨­è¨ˆç¢ºä¿æ¨¡å‹åŒæ™‚è€ƒæ…®ï¼š

| æå¤±é¡å‹ | å„ªåŒ–ç›®æ¨™ | å°æ‡‰äººçœ¼æ„ŸçŸ¥ |
|---------|---------|------------|
| **Charbonnier** | åƒç´ ç´šæº–ç¢ºæ€§ | æ•´é«”è‰²å½©ä¿çœŸåº¦ |
| **VGG Perceptual** | é«˜éšèªç¾©ç‰¹å¾µ | ç´‹ç†èˆ‡å…§å®¹ç›¸ä¼¼æ€§ |
| **Gradient** | é‚Šç·£éŠ³åˆ©åº¦ | ç´°ç¯€æ¸…æ™°ç¨‹åº¦ |
| **MS-SSIM** | å¤šå°ºåº¦çµæ§‹ç›¸ä¼¼æ€§ | æ•´é«”è¦–è¦ºå“è³ª |

### æå¤±å‡½æ•¸çµ„åˆå…¬å¼

$$\mathcal{L}_{total} = \sum_{i=1}^{4} w_i \cdot \mathcal{L}_i$$

å…¶ä¸­ $w_i$ æ˜¯å¯å­¸ç¿’çš„å‹•æ…‹æ¬Šé‡ï¼ˆé€é Softmax æ­£è¦åŒ–ï¼‰ã€‚

---

## æå¤±å‡½æ•¸æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Spectroformer æå¤±å‡½æ•¸æ¶æ§‹                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   è¼¸å…¥å½±åƒ (Input)              Ground Truth (GT)                           â”‚
â”‚        â”‚                             â”‚                                      â”‚
â”‚        â–¼                             â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚                                      â”‚
â”‚  â”‚ Spectro-  â”‚                       â”‚                                      â”‚
â”‚  â”‚  former   â”‚                       â”‚                                      â”‚
â”‚  â”‚   Model   â”‚                       â”‚                                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                       â”‚                                      â”‚
â”‚        â”‚                             â”‚                                      â”‚
â”‚        â–¼                             â–¼                                      â”‚
â”‚   é æ¸¬å½±åƒ (Pred)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º æå¤±è¨ˆç®—                                   â”‚
â”‚                                      â”‚                                      â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚        â”‚                             â”‚                              â”‚       â”‚
â”‚        â–¼                             â–¼                              â–¼       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚Charbonnierâ”‚                 â”‚    VGG    â”‚                 â”‚  Gradient â”‚  â”‚
â”‚  â”‚   Loss    â”‚                 â”‚ Perceptualâ”‚                 â”‚   Loss    â”‚  â”‚
â”‚  â”‚           â”‚                 â”‚   Loss    â”‚                 â”‚           â”‚  â”‚
â”‚  â”‚  åƒç´ ç´š   â”‚                 â”‚  ç‰¹å¾µç´š   â”‚                 â”‚  é‚Šç·£ç´š   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚                             â”‚                              â”‚       â”‚
â”‚        â”‚                             â”‚                              â”‚       â”‚
â”‚        â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                              â”‚       â”‚
â”‚        â”‚         â”‚  MS-SSIM  â”‚       â”‚                              â”‚       â”‚
â”‚        â”‚         â”‚   Loss    â”‚       â”‚                              â”‚       â”‚
â”‚        â”‚         â”‚           â”‚       â”‚                              â”‚       â”‚
â”‚        â”‚         â”‚  çµæ§‹ç´š   â”‚       â”‚                              â”‚       â”‚
â”‚        â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚                              â”‚       â”‚
â”‚        â”‚               â”‚             â”‚                              â”‚       â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                        â”‚             â”‚                                      â”‚
â”‚                        â–¼             â–¼                                      â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                â”‚     Weighted Loss       â”‚                                  â”‚
â”‚                â”‚                         â”‚                                  â”‚
â”‚                â”‚  wâ‚Â·L_char + wâ‚‚Â·L_per   â”‚                                  â”‚
â”‚                â”‚  + wâ‚ƒÂ·L_grad + wâ‚„Â·L_ssimâ”‚                                  â”‚
â”‚                â”‚                         â”‚                                  â”‚
â”‚                â”‚  â€» w_i ç‚ºå¯å­¸ç¿’æ¬Šé‡     â”‚                                  â”‚
â”‚                â”‚    (Softmax æ­£è¦åŒ–)     â”‚                                  â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                            â”‚                                                â”‚
â”‚                            â–¼                                                â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚                    â”‚  Total Loss   â”‚                                        â”‚
â”‚                    â”‚    L_total    â”‚                                        â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                            â”‚                                                â”‚
â”‚                            â–¼                                                â”‚
â”‚                     åå‘å‚³æ’­æ›´æ–°                                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å„æå¤±å‡½æ•¸è©³è§£

### 1. Charbonnier Loss

#### æ•¸å­¸å®šç¾©

$$\mathcal{L}_{Charbonnier} = \frac{1}{N} \sum_{i=1}^{N} \sqrt{(x_i - y_i)^2 + \epsilon^2}$$

å…¶ä¸­ï¼š
- $x_i$ï¼šé æ¸¬åƒç´ å€¼
- $y_i$ï¼šçœŸå¯¦åƒç´ å€¼
- $\epsilon$ï¼šç©©å®šå¸¸æ•¸ï¼ˆé è¨­ $\epsilon = 0.001$ï¼‰
- $N$ï¼šåƒç´ ç¸½æ•¸

#### è¨­è¨ˆåŸç†

Charbonnier Loss æ˜¯ L1 Loss çš„å¹³æ»‘è¿‘ä¼¼ç‰ˆæœ¬ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

1. **æ¢¯åº¦ç©©å®šæ€§**ï¼šç•¶ $|x - y| \rightarrow 0$ æ™‚ï¼ŒL1 çš„æ¢¯åº¦æœƒçªè®Šï¼ˆä¸å¯å¾®ï¼‰ï¼Œè€Œ Charbonnier ä¿æŒå¹³æ»‘
2. **é­¯æ£’æ€§**ï¼šå°æ–¼å¤§èª¤å·®ï¼ˆoutliersï¼‰çš„æ‡²ç½°æ¥è¿‘ L1ï¼Œæ¯” L2 æ›´é­¯æ£’
3. **æ”¶æ–‚æ€§**ï¼šå¹³æ»‘çš„æ¢¯åº¦ä½¿å„ªåŒ–éç¨‹æ›´ç©©å®š

#### èˆ‡å…¶ä»–æå¤±çš„æ¯”è¼ƒ

```
æ¢¯åº¦è¡Œç‚ºæ¯”è¼ƒï¼š

          L1 Loss                    L2 Loss                Charbonnier
       â”‚                          â”‚                          â”‚
     1 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚                          â”‚  ___________
       â”‚            â”€â”€â”€â”€â”€â”€       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”œâ”€â”€â”˜
   0.5 â”‚                         â”‚            â”€â”€           â”‚
       â”‚                         â”‚              â”€â”€         â”‚
     0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       â”‚            â”‚            â”‚             â”‚           â”‚          â”‚
  -0.5 â”‚            â”‚            â”‚          â”€â”€             â”‚
       â”‚            â”‚            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â””â”€â”€â”___________
    -1 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚                            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        -2    0     2             -2    0     2             -2    0     2
              èª¤å·®                      èª¤å·®                      èª¤å·®
```

#### ç¨‹å¼ç¢¼å¯¦ç¾

```python
class CharbonnierLoss(nn.Module):
    def __init__(self, eps=1e-3):
        super(CharbonnierLoss, self).__init__()
        self.eps = eps
        
    def forward(self, x, y):
        diff = x - y
        # sqrt((x-y)Â² + ÎµÂ²) ç¢ºä¿è™•è™•å¯å¾®
        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))
        return loss
```

#### ç‚ºä½•é¸æ“‡ Charbonnier è€Œé L1/L2ï¼Ÿ

| ç‰¹æ€§ | L1 | L2 | Charbonnier |
|-----|----|----|-------------|
| å° outliers çš„é­¯æ£’æ€§ | âœ… é«˜ | âŒ ä½ | âœ… é«˜ |
| æ¢¯åº¦å¹³æ»‘æ€§ | âŒ åœ¨ 0 è™•ä¸å¯å¾® | âœ… è™•è™•å¹³æ»‘ | âœ… è™•è™•å¹³æ»‘ |
| å°èª¤å·®æ‡²ç½° | ç·šæ€§ | äºŒæ¬¡æ–¹ï¼ˆéå°ï¼‰ | æ¥è¿‘ç·šæ€§ |
| æ”¶æ–‚ç©©å®šæ€§ | ä¸­ç­‰ | é«˜ | é«˜ |
| é©åˆå½±åƒä»»å‹™ | âœ… | â–³ | âœ…âœ… |

---

### 2. VGG Perceptual Loss

#### è¨­è¨ˆç†å¿µ

Perceptual Loss åŸºæ–¼ã€Œäººçœ¼æ„ŸçŸ¥ç›¸ä¼¼æ€§ä¸åƒ…ä¾è³´åƒç´ å·®ç•°ï¼Œæ›´ä¾è³´é«˜éšèªç¾©ç‰¹å¾µã€çš„è§€å¯Ÿã€‚åˆ©ç”¨é è¨“ç·´çš„ VGG19 ç¶²è·¯æå–ç‰¹å¾µï¼Œåœ¨ç‰¹å¾µç©ºé–“è¨ˆç®—è·é›¢ã€‚

#### æ•¸å­¸å®šç¾©

$$\mathcal{L}_{perceptual} = \frac{1}{C_j H_j W_j} \sum_{c=1}^{C_j} \sum_{h=1}^{H_j} \sum_{w=1}^{W_j} \left| \phi_j(x)_{c,h,w} - \phi_j(y)_{c,h,w} \right|$$

å…¶ä¸­ï¼š
- $\phi_j(x)$ï¼šVGG19 ç¬¬ $j$ å±¤çš„ç‰¹å¾µåœ–
- $C_j, H_j, W_j$ï¼šç‰¹å¾µåœ–çš„é€šé“æ•¸ã€é«˜åº¦ã€å¯¬åº¦

#### VGG19 æ¶æ§‹èˆ‡ç‰¹å¾µæå–

```
VGG19 æ¶æ§‹ï¼ˆæˆªå–è‡³ç¬¬ 35 å±¤ï¼‰
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

è¼¸å…¥: 3 Ã— 256 Ã— 256
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 1    â”‚  3 â†’ 64 channels
â”‚ Conv3-64 Ã— 2    â”‚  256 Ã— 256
â”‚ MaxPool         â”‚  â†’ 128 Ã— 128
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  â† ä½éšç‰¹å¾µï¼šé‚Šç·£ã€é¡è‰²
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 2    â”‚  64 â†’ 128 channels
â”‚ Conv3-128 Ã— 2   â”‚  128 Ã— 128
â”‚ MaxPool         â”‚  â†’ 64 Ã— 64
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  â† ç´‹ç†ç‰¹å¾µ
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 3    â”‚  128 â†’ 256 channels
â”‚ Conv3-256 Ã— 4   â”‚  64 Ã— 64
â”‚ MaxPool         â”‚  â†’ 32 Ã— 32
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  â† ä¸­éšç‰¹å¾µï¼šå½¢ç‹€ã€æ¨¡å¼
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 4    â”‚  256 â†’ 512 channels
â”‚ Conv3-512 Ã— 4   â”‚  32 Ã— 32
â”‚ MaxPool         â”‚  â†’ 16 Ã— 16
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  â† é«˜éšèªç¾©ç‰¹å¾µ
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conv Block 5    â”‚  512 â†’ 512 channels
â”‚ Conv3-512 Ã— 3   â”‚  16 Ã— 16
â”‚ (æˆªå–è‡³æ­¤)      â”‚  â† ä½¿ç”¨ features[:35]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    è¼¸å‡ºç‰¹å¾µåœ–
    512 Ã— 16 Ã— 16
```

#### ç‚ºä½•æˆªå–è‡³ç¬¬ 35 å±¤ï¼Ÿ

| å±¤ç´š | ç‰¹å¾µé¡å‹ | ç”¨é€” |
|-----|---------|------|
| æ—©æœŸå±¤ (1-10) | é‚Šç·£ã€é¡è‰² | éæ–¼ä½éšï¼Œèˆ‡åƒç´ æå¤±é‡è¤‡ |
| ä¸­æœŸå±¤ (11-25) | ç´‹ç†ã€æ¨¡å¼ | æœ‰æ„ç¾©ä½†ä¸å¤ æŠ½è±¡ |
| **å¾ŒæœŸå±¤ (26-35)** | **èªç¾©å…§å®¹** | **æœ€é©åˆæ„ŸçŸ¥ç›¸ä¼¼æ€§** |
| æ›´æ·±å±¤ (>35) | åˆ†é¡ç›¸é—œ | éæ–¼æŠ½è±¡ï¼Œå¤±å»ç©ºé–“è³‡è¨Š |

#### ç¨‹å¼ç¢¼å¯¦ç¾

```python
class VGGPerceptualLoss(nn.Module):
    def __init__(self):
        super(VGGPerceptualLoss, self).__init__()
        # è¼‰å…¥é è¨“ç·´ VGG19ï¼Œå–å‰ 35 å±¤
        vgg = vgg19(weights='DEFAULT').features[:35].eval().to(device)
        self.vgg = nn.Sequential(*vgg)
        
        # å‡çµ VGG åƒæ•¸ï¼ˆä¸åƒèˆ‡è¨“ç·´ï¼‰
        for param in self.vgg.parameters():
            param.requires_grad = False
            
    def forward(self, x, y):
        # åœ¨ç‰¹å¾µç©ºé–“è¨ˆç®— L1 è·é›¢
        return F.l1_loss(self.vgg(x), self.vgg(y))
```

#### è¨˜æ†¶é«”èˆ‡è¨ˆç®—è€ƒé‡

| é …ç›® | æ•¸å€¼ |
|-----|------|
| VGG19 åƒæ•¸é‡ | ~143Mï¼ˆä½†å‡çµä¸ä½”å„ªåŒ–å™¨è¨˜æ†¶é«”ï¼‰ |
| å‰å‘å‚³æ’­è¨˜æ†¶é«” | ~2GBï¼ˆbatch_size=4, 256Ã—256ï¼‰ |
| è¨ˆç®—æ™‚é–“ | ~15ms/batchï¼ˆTITAN RTXï¼‰ |

**æ³¨æ„**ï¼šVGG Perceptual Loss æ˜¯è¨“ç·´ä¸­è¨˜æ†¶é«”æ¶ˆè€—æœ€å¤§çš„å…ƒä»¶ï¼Œé€™ä¹Ÿæ˜¯ç‚ºä½• batch_size é™åˆ¶ç‚º 4ã€‚

---

### 3. Gradient Loss

#### è¨­è¨ˆç†å¿µ

Gradient Loss ç¢ºä¿é æ¸¬å½±åƒèˆ‡çœŸå¯¦å½±åƒåœ¨**é‚Šç·£çµæ§‹**ä¸Šä¸€è‡´ã€‚ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼ˆLaplacian operatorï¼‰æå–äºŒéšå°æ•¸è³‡è¨Šã€‚

#### æ‹‰æ™®æ‹‰æ–¯æ ¸ï¼ˆLaplacian Kernelï¼‰

ä½¿ç”¨çš„æ˜¯é›¢æ•£æ‹‰æ™®æ‹‰æ–¯æ ¸ï¼š

$$\nabla^2 = \begin{bmatrix} 0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0 \end{bmatrix}$$

#### æ•¸å­¸å®šç¾©

$$\mathcal{L}_{gradient} = \frac{1}{N} \sum_{i=1}^{N} \left| \nabla^2 x_i - \nabla^2 y_i \right|$$

å…¶ä¸­ $\nabla^2$ ä»£è¡¨æ‹‰æ™®æ‹‰æ–¯é‹ç®—ã€‚

#### ç‚ºä½•ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯è€Œé Sobelï¼Ÿ

| ç®—å­ | é¡å‹ | ç‰¹é» | é©ç”¨å ´æ™¯ |
|-----|------|------|---------|
| **Sobel** | ä¸€éšå°æ•¸ | æ–¹å‘æ€§ï¼ˆæ°´å¹³/å‚ç›´ï¼‰ | é‚Šç·£æª¢æ¸¬ |
| **Laplacian** | äºŒéšå°æ•¸ | å„å‘åŒæ€§ã€ç„¡æ–¹å‘ | é‚Šç·£éŠ³åˆ©åº¦ |

```
Sobel æ ¸ï¼ˆæ–¹å‘æ€§ï¼‰:           Laplacian æ ¸ï¼ˆå„å‘åŒæ€§ï¼‰:

Gx:                           
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚-1 â”‚ 0 â”‚+1 â”‚                â”‚ 0 â”‚ 1 â”‚ 0 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚-2 â”‚ 0 â”‚+2 â”‚                â”‚ 1 â”‚-4 â”‚ 1 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚-1 â”‚ 0 â”‚+1 â”‚                â”‚ 0 â”‚ 1 â”‚ 0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
   æ°´å¹³é‚Šç·£                     å…¨æ–¹å‘é‚Šç·£
```

#### ç¨‹å¼ç¢¼å¯¦ç¾

```python
class GradientLoss(nn.Module):
    def __init__(self):
        super(GradientLoss, self).__init__()
        # æ‹‰æ™®æ‹‰æ–¯æ ¸ï¼ˆ3 å€‹é€šé“å…±ç”¨ï¼‰
        kernel_g = [
            [[0, 1, 0], [1, -4, 1], [0, 1, 0]],
            [[0, 1, 0], [1, -4, 1], [0, 1, 0]],
            [[0, 1, 0], [1, -4, 1], [0, 1, 0]]
        ]
        kernel_g = torch.FloatTensor(kernel_g).unsqueeze(0).permute(1, 0, 2, 3)
        # å½¢ç‹€: (3, 1, 3, 3) ç”¨æ–¼ groups=3 çš„åˆ†çµ„å·ç©
        self.weight_g = nn.Parameter(data=kernel_g, requires_grad=False)

    def forward(self, x, xx):
        # ä½¿ç”¨ groups=3 å° RGB ä¸‰é€šé“åˆ†åˆ¥é€²è¡Œæ¢¯åº¦è¨ˆç®—
        gradient_x = F.conv2d(x, self.weight_g.to(x.device), groups=3)
        gradient_xx = F.conv2d(xx, self.weight_g.to(xx.device), groups=3)
        
        # æ¢¯åº¦åœ–çš„ L1 æå¤±
        l = nn.L1Loss()
        return l(gradient_x, gradient_xx)
```

#### è¦–è¦ºåŒ–æ•ˆæœ

```
åŸå§‹å½±åƒ:                    æ‹‰æ™®æ‹‰æ–¯éŸ¿æ‡‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚         â”‚    â–²            â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚   â†’     â”‚   â–² â–¼           â”‚
â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚         â”‚   â–² â–¼           â”‚
â”‚                 â”‚         â”‚    â–²            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  å‡å‹»å€åŸŸ+çŸ©å½¢ç‰©é«”            é‚Šç·£è™•é«˜éŸ¿æ‡‰
```

---

### 4. MS-SSIM Loss

#### è¨­è¨ˆç†å¿µ

MS-SSIM (Multi-Scale Structural Similarity) æ˜¯ SSIM çš„å¤šå°ºåº¦æ“´å±•ï¼Œæ¨¡æ“¬äººçœ¼åœ¨ä¸åŒè§€å¯Ÿè·é›¢ä¸‹çš„è¦–è¦ºæ„ŸçŸ¥ã€‚

#### æ•¸å­¸å®šç¾©

å–®å°ºåº¦ SSIMï¼š

$$SSIM(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}$$

å¤šå°ºåº¦ MS-SSIMï¼š

$$MS\text{-}SSIM(x, y) = [l_M(x, y)]^{\alpha_M} \cdot \prod_{j=1}^{M} [cs_j(x, y)]^{\beta_j}$$

å…¶ä¸­ï¼š
- $l_M$ï¼šæœ€ç²—å°ºåº¦çš„äº®åº¦æ¯”è¼ƒ
- $cs_j$ï¼šç¬¬ $j$ å°ºåº¦çš„å°æ¯”åº¦-çµæ§‹æ¯”è¼ƒ
- $\alpha_M, \beta_j$ï¼šå„å°ºåº¦çš„æ¬Šé‡

#### å¤šå°ºåº¦æ¶æ§‹

```
MS-SSIM å¤šå°ºåº¦è¨ˆç®—æµç¨‹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

          é æ¸¬å½±åƒ x                    çœŸå¯¦å½±åƒ y
              â”‚                             â”‚
              â–¼                             â–¼
         Scale 1 (åŸå§‹å°ºåº¦ 256Ã—256)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  è¨ˆç®— contrast Ã— structure = csâ‚   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ â†“ 2Ã— ä¸‹æ¡æ¨£
                           â–¼
         Scale 2 (128Ã—128)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  è¨ˆç®— contrast Ã— structure = csâ‚‚   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ â†“ 2Ã— ä¸‹æ¡æ¨£
                           â–¼
         Scale 3 (64Ã—64)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  è¨ˆç®— contrast Ã— structure = csâ‚ƒ   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ â†“ 2Ã— ä¸‹æ¡æ¨£
                           â–¼
         Scale 4 (32Ã—32)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  è¨ˆç®— contrast Ã— structure = csâ‚„   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ â†“ 2Ã— ä¸‹æ¡æ¨£
                           â–¼
         Scale 5 (16Ã—16) - æœ€ç²—å°ºåº¦
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  è¨ˆç®— luminance Ã— contrast Ã—       â”‚
         â”‚        structure = lâ‚… Ã— csâ‚…        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     MS-SSIM = lâ‚…^Î± Ã— âˆ(csâ±¼^Î²â±¼)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### åƒæ•¸è¨­å®š

```python
MS_SSIM_loss = MS_SSIM(
    win_size=11,      # é«˜æ–¯çª—å£å¤§å°
    win_sigma=1.5,    # é«˜æ–¯æ¨™æº–å·®
    data_range=1,     # åƒç´ å€¼ç¯„åœ [0, 1]
    size_average=True,# è¿”å›å¹³å‡å€¼
    channel=3         # RGB ä¸‰é€šé“
).to(device)
```

#### è½‰æ›ç‚ºæå¤±

ç”±æ–¼ MS-SSIM çš„ç¯„åœæ˜¯ $[0, 1]$ï¼Œå€¼è¶Šå¤§è¡¨ç¤ºè¶Šç›¸ä¼¼ï¼Œå› æ­¤éœ€è¦è½‰æ›ï¼š

$$\mathcal{L}_{MS\text{-}SSIM} = 1 - MS\text{-}SSIM(x, y)$$

#### ç‚ºä½•ä½¿ç”¨ MS-SSIM è€Œé SSIMï¼Ÿ

| ç‰¹æ€§ | SSIM | MS-SSIM |
|-----|------|---------|
| å°ºåº¦æ•æ„Ÿæ€§ | å–®ä¸€å›ºå®šå°ºåº¦ | å¤šå°ºåº¦æ•´åˆ |
| æ¨¡æ“¬äººçœ¼ | å–®ä¸€è§€å¯Ÿè·é›¢ | å¤šè§€å¯Ÿè·é›¢ |
| å°æ¨¡ç³Šçš„æ•æ„Ÿåº¦ | ä¸€èˆ¬ | æ›´æ•æ„Ÿ |
| è¨ˆç®—æˆæœ¬ | è¼ƒä½ | è¼ƒé«˜ï¼ˆ5Ã— ä¸‹æ¡æ¨£ï¼‰ |
| å­¸è¡“èªå¯ | å»£æ³›ä½¿ç”¨ | æ›´ç¬¦åˆä¸»è§€è©•åƒ¹ |

---

## å‹•æ…‹åŠ æ¬Šæ©Ÿåˆ¶

### WeightedLoss é¡åˆ¥

```python
class WeightedLoss(nn.Module):
    """å‹•æ…‹åŠ æ¬Šæå¤±çµ„åˆå™¨"""
    
    def __init__(self, num_weights):
        super(WeightedLoss, self).__init__()
        self.num_weights = num_weights
        # å¯å­¸ç¿’çš„æ¬Šé‡åƒæ•¸ï¼ˆéš¨æ©Ÿåˆå§‹åŒ–ï¼‰
        self.weights = nn.Parameter(torch.rand(1, num_weights))
        self.softmax_l = nn.Softmax(dim=1)

    def forward(self, *argv):
        loss = 0
        # ä½¿ç”¨ Softmax ç¢ºä¿æ¬Šé‡å’Œç‚º 1
        weights = self.softmax_l(self.weights)
        for idx, arg in enumerate(argv):
            loss += arg * weights[0, idx]
        return loss
```

### è¨­è¨ˆåŸç†

```
å‹•æ…‹åŠ æ¬Šæ©Ÿåˆ¶æµç¨‹åœ–
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    å¯å­¸ç¿’æ¬Šé‡ w (éš¨æ©Ÿåˆå§‹åŒ–)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  w = [wâ‚_raw, wâ‚‚_raw, wâ‚ƒ_raw, wâ‚„_raw]  â”‚
    â”‚      (nn.Parameter)                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼ Softmax
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  [wâ‚, wâ‚‚, wâ‚ƒ, wâ‚„] = Softmax(w)   â”‚
    â”‚                                   â”‚
    â”‚  ç¢ºä¿: wâ‚ + wâ‚‚ + wâ‚ƒ + wâ‚„ = 1    â”‚
    â”‚  ä¸”:   wáµ¢ âˆˆ (0, 1)               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                       â”‚
    â”‚  L_total = wâ‚Â·L_char + wâ‚‚Â·L_per + wâ‚ƒÂ·L_grad + wâ‚„Â·L_ssim â”‚
    â”‚                                                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚  â”‚ Ã—0.25  â”‚ â”‚ Ã—0.35  â”‚ â”‚ Ã—0.15  â”‚ â”‚ Ã—0.25  â”‚ (ç¤ºä¾‹)  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚      â†‘          â†‘          â†‘          â†‘              â”‚
    â”‚   L_char    L_per     L_grad    L_ssim               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç‚ºä½•ä½¿ç”¨å‹•æ…‹æ¬Šé‡ï¼Ÿ

1. **è‡ªå‹•å¹³è¡¡**ï¼šä¸åŒæå¤±çš„æ•¸å€¼ç¯„åœå·®ç•°å¤§ï¼Œæ‰‹å‹•èª¿æ•´å›°é›£
2. **ä»»å‹™é©æ‡‰**ï¼šæ¨¡å‹è‡ªå‹•å­¸ç¿’å„æå¤±çš„ç›¸å°é‡è¦æ€§
3. **è¨“ç·´ç©©å®š**ï¼šé¿å…æŸä¸€æå¤±ä¸»å°ï¼Œå°è‡´å…¶ä»–æå¤±è¢«å¿½ç•¥

### å…¸å‹æ¬Šé‡æ¼”è®Š

```
è¨“ç·´éç¨‹ä¸­æ¬Šé‡çš„æ¼”è®Šï¼ˆç¤ºæ„ï¼‰:

 0.5 â”¤
     â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€ Charbonnier
 0.4 â”¤   â•±
     â”‚  â•±   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Perceptual
 0.3 â”¤ â•±   â•±
     â”‚â•±   â•±
 0.2 â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradient
     â”‚            â•²
 0.1 â”¤             â•²â”€â”€â”€â”€â”€â”€ MS-SSIM
     â”‚
 0.0 â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–º
     0   20   40   60   80  100
                Epochs
```

**è¨“ç·´åˆæœŸ**ï¼šCharbonnier ä½”ä¸»å°ï¼ˆå¿«é€Ÿæ”¶æ–‚åƒç´ ç´šèª¤å·®ï¼‰
**è¨“ç·´ä¸­æœŸ**ï¼šPerceptual æ¬Šé‡ä¸Šå‡ï¼ˆå„ªåŒ–èªç¾©ç‰¹å¾µï¼‰
**è¨“ç·´å¾ŒæœŸ**ï¼šæ¬Šé‡è¶¨æ–¼ç©©å®š

---

## æ•¸å­¸æ¨å°èˆ‡å…¬å¼

### ç¸½é«”æå¤±å‡½æ•¸

$$\mathcal{L}_{total} = \text{Softmax}(\mathbf{w}) \cdot \begin{bmatrix} \mathcal{L}_{char} \\ \mathcal{L}_{per} \\ \mathcal{L}_{grad} \\ \mathcal{L}_{ssim} \end{bmatrix}$$

å±•é–‹ç‚ºï¼š

$$\mathcal{L}_{total} = \frac{e^{w_1}}{\sum_{j=1}^{4} e^{w_j}} \mathcal{L}_{char} + \frac{e^{w_2}}{\sum_{j=1}^{4} e^{w_j}} \mathcal{L}_{per} + \frac{e^{w_3}}{\sum_{j=1}^{4} e^{w_j}} \mathcal{L}_{grad} + \frac{e^{w_4}}{\sum_{j=1}^{4} e^{w_j}} \mathcal{L}_{ssim}$$

### å„æå¤±çš„æ¢¯åº¦

å°æ–¼é æ¸¬åƒç´  $\hat{x}$ï¼š

$$\frac{\partial \mathcal{L}_{total}}{\partial \hat{x}} = \sum_{i=1}^{4} \tilde{w}_i \cdot \frac{\partial \mathcal{L}_i}{\partial \hat{x}}$$

å…¶ä¸­ $\tilde{w}_i = \text{Softmax}(w_i)$ã€‚

---

## æå¤±å‡½æ•¸æ¯”è¼ƒè¡¨

| æå¤±å‡½æ•¸ | å„ªåŒ–ç›®æ¨™ | æ•¸å­¸å½¢å¼ | è¨ˆç®—è¤‡é›œåº¦ | è¨˜æ†¶é«”éœ€æ±‚ | å…¸å‹å€¼ç¯„åœ |
|---------|---------|---------|-----------|----------|-----------|
| **Charbonnier** | åƒç´ æº–ç¢º | $\sqrt{(x-y)^2 + \epsilon^2}$ | $O(N)$ | ä½ | 0.001 - 0.1 |
| **VGG Perceptual** | èªç¾©ç›¸ä¼¼ | $\|VGG(x) - VGG(y)\|_1$ | $O(N \cdot VGG)$ | **é«˜** | 0.01 - 0.5 |
| **Gradient** | é‚Šç·£éŠ³åˆ© | $\|\nabla^2 x - \nabla^2 y\|_1$ | $O(N)$ | ä½ | 0.001 - 0.05 |
| **MS-SSIM** | çµæ§‹ç›¸ä¼¼ | $1 - MS\text{-}SSIM(x, y)$ | $O(5N)$ | ä¸­ | 0.01 - 0.3 |

---

## è¨“ç·´ä¸­çš„æå¤±è¡Œç‚ºåˆ†æ

### ç†æƒ³çš„æå¤±æ›²ç·š

```
æå¤±å€¼
   â”‚
0.5â”¤     â•²
   â”‚      â•²  
0.4â”¤       â•²              
   â”‚        â•²            
0.3â”¤         â•²           â† å¿«é€Ÿä¸‹é™æœŸ (Epoch 0-20)
   â”‚          â•²          
0.2â”¤           â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€  
   â”‚                 â”€â”€â”€â”€â”€â”€ â† ç©©å®šä¸‹é™æœŸ (Epoch 20-100)
0.1â”¤                     â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚                           â”€â”€â”€â”€â”€â”€â”€â”€â”€ â† æ”¶æ–‚æœŸ (Epoch 100+)
0.0â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–º
   0   20   40   60   80  100  120
                   Epochs
```

### ç•°å¸¸æå¤±æ¨¡å¼è¨ºæ–·

| æ¨¡å¼ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|-----|---------|---------|
| æå¤±éœ‡ç›ªåŠ‡çƒˆ | å­¸ç¿’ç‡éé«˜ | é™ä½ `--lr` |
| æå¤±ä¸ä¸‹é™ | å­¸ç¿’ç‡éä½æˆ–æ¢¯åº¦æ¶ˆå¤± | æé«˜ `--lr` æˆ–æª¢æŸ¥æ¨¡å‹çµæ§‹ |
| æå¤±çˆ†ç‚¸ (NaN) | æ•¸å€¼ä¸ç©©å®š | é™ä½å­¸ç¿’ç‡ã€æ·»åŠ æ¢¯åº¦è£å‰ª |
| æŸä¸€æå¤±ä¸»å° | æ¬Šé‡ä¸å¹³è¡¡ | èª¿æ•´åˆå§‹åŒ–æˆ–ä½¿ç”¨å‹•æ…‹æ¬Šé‡ |
| è¨“ç·´/é©—è­‰å·®è·å¤§ | éæ“¬åˆ | å¢åŠ è³‡æ–™å¢å¼·æˆ–æ­£å‰‡åŒ– |

---

## è¶…åƒæ•¸å»ºè­°

### ç•¶å‰è¨­å®š

```python
# å­¸ç¿’ç‡
opt.lr = 0.0001  # å»ºè­°ç¯„åœï¼š0.00001 - 0.001

# Charbonnier Îµ
eps = 1e-3  # å»ºè­°ï¼š1e-4 åˆ° 1e-2

# MS-SSIM åƒæ•¸
win_size = 11  # æ¨™æº–è¨­å®š
win_sigma = 1.5  # æ¨™æº–è¨­å®š

# VGG å±¤æ•¸
features[:35]  # æ¨™æº–è¨­å®šï¼ˆç¬¬ 5 å€‹å·ç©å¡Šï¼‰
```

### ä¸åŒå ´æ™¯çš„èª¿æ•´å»ºè­°

| å ´æ™¯ | Charbonnier | Perceptual | Gradient | MS-SSIM |
|-----|-------------|------------|----------|---------|
| è¿½æ±‚ PSNR | æ¬Šé‡è¼ƒé«˜ | æ¬Šé‡è¼ƒä½ | ä¸­ç­‰ | ä¸­ç­‰ |
| è¿½æ±‚è¦–è¦ºå“è³ª | æ¬Šé‡è¼ƒä½ | **æ¬Šé‡è¼ƒé«˜** | ä¸­ç­‰ | **æ¬Šé‡è¼ƒé«˜** |
| é‚Šç·£éŠ³åˆ©åŒ– | ä¸­ç­‰ | ä¸­ç­‰ | **æ¬Šé‡è¼ƒé«˜** | ä¸­ç­‰ |
| å¿«é€Ÿè¨“ç·´ | ä¿ç•™ | **å¯ç§»é™¤** | ä¿ç•™ | ä¿ç•™ |

---

## ç¨‹å¼ç¢¼å¯¦ç¾è©³è§£

### å®Œæ•´æå¤±è¨ˆç®—æµç¨‹

```python
# 1. åˆå§‹åŒ–æ‰€æœ‰æå¤±å‡½æ•¸
Charbonnier_loss = CharbonnierLoss().to(device)  # åƒç´ ç´š
Gradient_Loss = GradientLoss().to(device)        # é‚Šç·£ç´š
L_per = VGGPerceptualLoss().to(device)           # ç‰¹å¾µç´š
MS_SSIM_loss = MS_SSIM(                          # çµæ§‹ç´š
    win_size=11, 
    win_sigma=1.5, 
    data_range=1, 
    size_average=True, 
    channel=3
).to(device)

# 2. åˆå§‹åŒ–å‹•æ…‹æ¬Šé‡
Weighted_Loss4 = WeightedLoss(4).to(device)

# 3. è¨“ç·´è¿´åœˆä¸­çš„æå¤±è¨ˆç®—
for batch in training_data_loader:
    rgb, tar = batch[0].to(device), batch[1].to(device)
    
    # å‰å‘å‚³æ’­
    fake_b = net_g(rgb)
    
    # è¨ˆç®—å„å€‹æå¤±
    loss_char = Charbonnier_loss(tar, fake_b)      # åƒç´ é‡å»º
    loss_per = L_per(fake_b, tar)                  # æ„ŸçŸ¥ç›¸ä¼¼
    loss_grad = Gradient_Loss(fake_b, tar)         # é‚Šç·£ä¿æŒ
    loss_ssim = 1 - MS_SSIM_loss(fake_b, tar)      # çµæ§‹ç›¸ä¼¼
    
    # å‹•æ…‹åŠ æ¬Šçµ„åˆ
    loss_g = Weighted_Loss4(loss_char, loss_per, loss_grad, loss_ssim)
    
    # ç•°å¸¸æª¢æ¸¬
    if torch.isnan(loss_g) or torch.isinf(loss_g):
        print("è­¦å‘Šï¼šæª¢æ¸¬åˆ°ç•°å¸¸æå¤±")
        continue
    
    # åå‘å‚³æ’­
    loss_g.backward()
    
    # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰
    torch.nn.utils.clip_grad_norm_(net_g.parameters(), max_norm=1.0)
    
    optimizer_g.step()
```

### è¨˜æ†¶é«”å„ªåŒ–æŠ€å·§

```python
# 1. VGG è¨­ç‚º eval æ¨¡å¼ï¼Œæ¸›å°‘ä¸å¿…è¦çš„çµ±è¨ˆé‡è¿½è¹¤
vgg = vgg19(weights='DEFAULT').features[:35].eval().to(device)

# 2. å‡çµ VGG åƒæ•¸
for param in self.vgg.parameters():
    param.requires_grad = False

# 3. ä½¿ç”¨ torch.no_grad() é€²è¡Œé©—è­‰
with torch.no_grad():
    prediction = net_g(rgb_input)
```

---

## ç¸½çµ

Spectroformer çš„æå¤±å‡½æ•¸è¨­è¨ˆé«”ç¾äº†ç¾ä»£æ·±åº¦å­¸ç¿’å½±åƒè™•ç†çš„æœ€ä½³å¯¦è¸ï¼š

1. **å¤šå±¤æ¬¡ç›£ç£**ï¼šå¾åƒç´ ç´šåˆ°èªç¾©ç´šçš„å…¨æ–¹ä½ç´„æŸ
2. **å‹•æ…‹å¹³è¡¡**ï¼šå¯å­¸ç¿’æ¬Šé‡è‡ªå‹•èª¿æ•´å„æå¤±è²¢ç»
3. **é­¯æ£’è¨“ç·´**ï¼šCharbonnier å’Œæ¢¯åº¦è£å‰ªç¢ºä¿ç©©å®šæ€§
4. **æ„ŸçŸ¥å„ªå…ˆ**ï¼šVGG å’Œ MS-SSIM ç¢ºä¿è¦–è¦ºå“è³ª

é€™å¥—æå¤±å‡½æ•¸çµ„åˆç‰¹åˆ¥é©åˆæ°´ä¸‹å½±åƒå¢å¼·ä»»å‹™ï¼Œå› ç‚ºï¼š
- æ°´ä¸‹å½±åƒè‰²å½©å¤±çœŸåš´é‡ â†’ Charbonnier æ¢å¾©è‰²å½©
- ç´°ç¯€æ¨¡ç³Š â†’ Gradient Loss å¼·åŒ–é‚Šç·£
- çµæ§‹æ‰­æ›² â†’ MS-SSIM ä¿æŒçµæ§‹
- æ•´é«”è¦–è¦ºå“è³ª â†’ VGG Perceptual å„ªåŒ–æ„ŸçŸ¥

---

## é™„éŒ„ï¼šç›¸é—œæ–‡ç»

1. **Charbonnier Loss**: Charbonnier, P. et al. "Two deterministic half-quadratic regularization algorithms for computed imaging." ICIP 1994.
2. **Perceptual Loss**: Johnson, J. et al. "Perceptual Losses for Real-Time Style Transfer and Super-Resolution." ECCV 2016.
3. **MS-SSIM**: Wang, Z. et al. "Multi-scale structural similarity for image quality assessment." Asilomar 2003.
4. **Gradient Loss**: Mathieu, M. et al. "Deep multi-scale video prediction beyond mean square error." ICLR 2016.

---

*å ±å‘Šç”Ÿæˆæ™‚é–“: 2024*
*é©ç”¨ç‰ˆæœ¬: trains.py (Spectroformer è¨“ç·´è…³æœ¬)*
