# Phaseformer è¨“ç·´æŒ‡å—

## ğŸ”§ ä¸»è¦ä¿®æ”¹èªªæ˜

### 1. æ¨¡å‹æ¶æ§‹å·®ç•°

#### èˆŠæ¨¡å‹ (final_model_AGSSF1)
- **è¼¸å‡º**: å–®ä¸€è¼¸å‡º (256Ã—256)
- **æ³¨æ„åŠ›æ©Ÿåˆ¶**: SFCA (Spatial-Frequency Channel Attention)
- **ä¸Šæ¡æ¨£**: é »åŸŸä¸Šæ¡æ¨£ (UpS)

#### æ–°æ¨¡å‹ (Restormer/Phaseformer)
- **è¼¸å‡º**: é›™è¼¸å‡º
  - `fake_b_L`: ä½è§£æåº¦è¼¸å‡º (256Ã—256)
  - `fake_b_H`: é«˜è§£æåº¦è¼¸å‡º (512Ã—512)
- **æ³¨æ„åŠ›æ©Ÿåˆ¶**: ECA (Efficient Channel Attention)
- **ä¸Šæ¡æ¨£**: åƒç´ ä¸Šæ¡æ¨£ (PixelShuffle)

### 2. è¨“ç·´æå¤±è¨ˆç®—

```python
# ä½è§£æåº¦åˆ†æ”¯æå¤± (256Ã—256)
loss_g_L = Weighted_Loss4(
    Charbonnier_loss(tar, fake_b_L),
    L_per(fake_b_L, tar),
    Gradient_Loss(fake_b_L, tar),
    1 - MS_SSIM_loss(fake_b_L, tar)
)

# é«˜è§£æåº¦åˆ†æ”¯æå¤± (512Ã—512)
tar_H = F.interpolate(tar, scale_factor=2, mode='bilinear')
loss_g_H = Weighted_Loss4(
    Charbonnier_loss(tar_H, fake_b_H),
    L_per(fake_b_H, tar_H),
    Gradient_Loss(fake_b_H, tar_H),
    1 - MS_SSIM_loss(fake_b_H, tar_H)
)

# æœ€çµ‚æå¤±ï¼ˆå‹•æ…‹æ¬Šé‡çµ„åˆï¼‰
loss_g = Weighted_Loss2(loss_g_L, loss_g_H)
```

### 3. å‹•æ…‹æ¬Šé‡æ©Ÿåˆ¶ (WeightedLoss)

```python
class WeightedLoss(nn.Module):
    """
    å¯å­¸ç¿’çš„æå¤±æ¬Šé‡
    - ä½¿ç”¨ softmax ç¢ºä¿æ¬Šé‡å’Œç‚º 1
    - æ¬Šé‡åœ¨è¨“ç·´éç¨‹ä¸­è‡ªå‹•èª¿æ•´
    """
    def __init__(self, num_weights):
        super(WeightedLoss, self).__init__()
        self.weights = nn.Parameter(torch.rand(1, num_weights))
        self.softmax_l = nn.Softmax(dim=1)
```

**å„ªé»:**
- è‡ªå‹•å¹³è¡¡ä¸åŒæå¤±çµ„ä»¶
- é¿å…æ‰‹å‹•èª¿æ•´æ¬Šé‡
- æ›´å¥½çš„è¨“ç·´ç©©å®šæ€§

## ğŸ“Š èˆ‡åŸå§‹ train.py çš„å°æ¯”

| ç‰¹æ€§ | train.py (åŸå§‹) | trains.py (ä¿®æ”¹å¾Œ) |
|------|----------------|-------------------|
| æ•¸æ“šé›† | å–®ä¸€æ•¸æ“šé›† | æ”¯æŒå¤šæ•¸æ“šé›† |
| GPU æ”¯æŒ | å–®å¡ | å¤šå¡ä¸¦è¡Œ (DataParallel) |
| æ•¸æ“šå¢å¼· | é›™å°ºåº¦ (256, 512) | å–®å°ºåº¦ (256) + å‹•æ…‹ä¸Šæ¡æ¨£ |
| æª¢æŸ¥é»ä¿å­˜ | ä¿å­˜æ•´å€‹æ¨¡å‹ | ä¿å­˜ state_dict |
| é€²åº¦æ¢ | ç„¡ | tqdm é€²åº¦æ¢ |
| æå¤±è¨ˆç®— | âœ… é›™è¼¸å‡ºæå¤± | âœ… é›™è¼¸å‡ºæå¤± |

## ğŸš€ åœ¨æœå‹™å™¨ä¸ŠåŸ·è¡Œ

### 1. ç’°å¢ƒæº–å‚™

```bash
# å®‰è£å¿…è¦å¥—ä»¶
pip install pytorch-msssim
pip install scikit-image
pip install tqdm
pip install kornia
```

### 2. æª¢æŸ¥æ•¸æ“šé›†çµæ§‹

```
/danny/Spectroformer_model/LSUI/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ gt/
â””â”€â”€ test/
    â”œâ”€â”€ input/
    â””â”€â”€ gt/
```

### 3. é–‹å§‹è¨“ç·´

```bash
# å¾é ­è¨“ç·´
python trains.py \
    --dataset_path /danny/Spectroformer_model/LSUI \
    --batch_size 8 \
    --niter 50 \
    --niter_decay 500 \
    --lr 0.001

# æ¢å¾©è¨“ç·´ï¼ˆæ³¨æ„ï¼šæ–°èˆŠæ¨¡å‹çµæ§‹ä¸åŒï¼Œç„¡æ³•ç›´æ¥è¼‰å…¥èˆŠæª¢æŸ¥é»ï¼‰
python trains.py \
    --resume checkpoint/LSUI/netG_model_epoch_XXX.pth \
    --dataset_path /danny/Spectroformer_model/LSUI
```

### 4. ç›£æ§è¨“ç·´

è¨“ç·´éç¨‹ä¸­æœƒé¡¯ç¤ºï¼š
- âœ… å³æ™‚æå¤±å€¼ (Loss)
- âœ… å¹³å‡æå¤± (Avg_Loss)
- âœ… ç•¶å‰å­¸ç¿’ç‡ (LR)
- âœ… è¨“ç·´é€²åº¦ (tqdm é€²åº¦æ¢)
- âœ… æ¸¬è©¦ PSNR å’Œ SSIM

## âš ï¸ é‡è¦æ³¨æ„äº‹é …

### 1. æª¢æŸ¥é»ä¸å…¼å®¹
**å•é¡Œ**: æ–°èˆŠæ¨¡å‹çµæ§‹å®Œå…¨ä¸åŒ
**è§£æ±º**: 
- âŒ ä¸èƒ½ä½¿ç”¨ `--resume` è¼‰å…¥èˆŠæ¨¡å‹æª¢æŸ¥é»
- âœ… éœ€è¦å¾é ­é–‹å§‹è¨“ç·´

### 2. é¡¯å­˜ä½¿ç”¨
ç”±æ–¼é›™è¼¸å‡ºæå¤±è¨ˆç®—ï¼Œé¡¯å­˜ä½¿ç”¨å¢åŠ ç´„ **40-50%**

**å»ºè­°**:
- æ¸›å°æ‰¹æ¬¡å¤§å° (ä¾‹å¦‚å¾ 8 â†’ 4)
- æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç©

```python
# æ¢¯åº¦ç´¯ç©ç¤ºä¾‹ï¼ˆåœ¨ trains.py ä¸­æ·»åŠ ï¼‰
accumulation_steps = 2
for i, batch in enumerate(dataloader):
    loss = loss / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 3. å­¸ç¿’ç‡èª¿æ•´
åŸå§‹æ¨¡å‹åƒæ•¸é‡å¯èƒ½ä¸åŒï¼Œå»ºè­°ï¼š
- åˆå§‹å­¸ç¿’ç‡: 0.0001 - 0.001
- ä½¿ç”¨å­¸ç¿’ç‡é ç†± (warmup)

### 4. æå¤±æ¬Šé‡ç›£æ§
å¯ä»¥æŸ¥çœ‹å‹•æ…‹æ¬Šé‡çš„è®ŠåŒ–ï¼š
```python
# åœ¨è¨“ç·´å¾ªç’°ä¸­æ·»åŠ 
print(f"Læå¤±æ¬Šé‡: {Weighted_Loss4.weights.softmax(1)}")
print(f"L/Hæå¤±æ¬Šé‡: {Weighted_Loss2.weights.softmax(1)}")
```

## ğŸ¯ è¨“ç·´æŠ€å·§

### 1. å¤šå°ºåº¦è¨“ç·´
è€ƒæ…®ä½¿ç”¨ä¸åŒå°ºåº¦çš„è¼¸å…¥ï¼š
```python
scales = [0.8, 1.0, 1.2]
for scale in scales:
    scaled_input = F.interpolate(input, scale_factor=scale)
    output = model(scaled_input)
```

### 2. æ··åˆç²¾åº¦è¨“ç·´ (AMP)
æ¸›å°‘é¡¯å­˜ä½¿ç”¨ï¼ŒåŠ é€Ÿè¨“ç·´ï¼š
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. å­¸ç¿’ç‡èª¿åº¦
```python
# Cosine Annealing
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)

# One Cycle
scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, total_steps=epochs)
```

## ğŸ“ˆ é æœŸçµæœ

æ ¹æ“šåŸå§‹è«–æ–‡å’Œæ¸¬è©¦çµæœï¼š

| Epoch | PSNR (dB) | SSIM | èªªæ˜ |
|-------|-----------|------|------|
| 100   | 26-28     | 0.88-0.90 | åˆæœŸè¨“ç·´ |
| 300   | 28-29     | 0.90-0.92 | ä¸­æœŸè¨“ç·´ |
| 500+  | 29-30     | 0.92-0.93 | æ”¶æ–‚éšæ®µ |

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: RuntimeError: CUDA out of memory
**è§£æ±º**:
```bash
# æ¸›å°æ‰¹æ¬¡å¤§å°
--batch_size 4

# æˆ–æ¸›å°åœ–ç‰‡å°ºå¯¸ï¼ˆä¿®æ”¹ transforms.Resizeï¼‰
transforms.Resize((128, 128))
```

### Q2: æå¤±è®Šæˆ NaN
**è§£æ±º**:
- æª¢æŸ¥å­¸ç¿’ç‡æ˜¯å¦éå¤§
- å•Ÿç”¨æ¢¯åº¦è£å‰ªï¼ˆå·²åŒ…å«åœ¨ trains.py ä¸­ï¼‰
- æª¢æŸ¥æ•¸æ“šæ­¸ä¸€åŒ–

### Q3: è¨“ç·´é€Ÿåº¦æ…¢
**è§£æ±º**:
```python
# å¢åŠ  num_workers
--threads 4

# å•Ÿç”¨ cudnn benchmarkï¼ˆå·²åŒ…å«ï¼‰
cudnn.benchmark = True
```

## ğŸ“ ä¿®æ”¹è¨˜éŒ„

**2025-11-02**:
- âœ… ä¿®å¾©é›™è¼¸å‡ºè™•ç†
- âœ… æ·»åŠ å‹•æ…‹æ¬Šé‡æå¤±
- âœ… åƒè€ƒ train.py å¯¦ç¾é›™åˆ†æ”¯æå¤±
- âœ… æ·»åŠ è©³ç´°çš„éŒ¯èª¤æª¢æ¸¬å’Œæ—¥èªŒ

## ğŸ“š åƒè€ƒè³‡æ–™

1. åŸå§‹ Restormer è«–æ–‡
2. train.py å¯¦ç¾
3. LSUI æ•¸æ“šé›†è¦æ ¼
